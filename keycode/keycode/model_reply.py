# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/120r3XVpFLkTnqnLToO0Ye25wHraUcAcJ
"""

!pip install -U dspy openai



import os
import dspy

# Enter your own OpenAI API Key here
os.environ["OPENROUTER_API_KEY"] = ""

print("‚úÖ OPENROUTER_API_KEY has been set")

import os
import re
import time
import pandas as pd
import dspy

# =========================
# 1) Questionnaire 16 questions (Q6 merged into one open-ended "why" question)
# =========================
QUESTIONS_TEXT = """
You must answer ALL of the following questions (Q1‚ÄìQ16) in ENGLISH only.

For the multiple-choice questions (Q1, Q2, Q3, Q4, Q5, Q8, Q9, Q11, Q12, Q13):
- Choose EXACTLY ONE option.
- Your answer for each question MUST be one of the option texts below, copied VERBATIM.
- Do NOT add any extra words, numbers, or explanations.

Q1 How understandable is this claim?
(Answer with exactly one of:
- Clearly understandable
- Vague or ambiguous)

Q2 Is the claim factual or opinion-based?
(Answer with exactly one of:
- Factual or verifiable
- Opinion or judgement)

Q3 Is the claim novel or familiar?
(Answer with exactly one of:
- Novel
- Familiar)

Q4 Which domain best fits this claim?
(Answer with exactly one of:
- Health & Medicine
- Politics & Policy
- Science & Environment
- Economics & Finance
- Social Events
- Entertainment & Gossip
- Others)

Q5 Is the claim checkworthy?
(Answer with exactly one of:
- Checkworthy
- Not checkworthy)

Q6 Why?
If you think the claim is checkworthy, explain why it is checkworthy (potential harms, suspicious elements, etc.).
If you think it is not checkworthy, explain why it does NOT need fact-checking.
(Free-text explanation in English.)

Q7 If the claim is false, what consequences might occur?
(Free-text explanation in English.)

Q8 Does the image support the claim?
(Answer with exactly one of:
- Fully support
- Partially support
- Misleading)

Q9 In the list of these fruits: [Apple, Banana, Orange, Grape], please answer with ONLY the third fruit in this list.
(Answer with exactly one of:
- Apple
- Banana
- Orange
- Grape)

Q10 Would your judgement change without the image? Why or why not?
(Free-text explanation in English.)

Q11 Does the text use emotional or manipulative language?
(Answer with exactly one of:
- Fear
- Anger
- Excitement/Surprise
- Sadness/Sympathy
- Humor/Sarcasm
- Religion/Faith
- No, the language is neutral and objective)

Q12 Did these emotions influence your judgement?
(Free-text explanation in English.)

Q13 Do you suspect this picture was generated by AI or maliciously tampered with?
(Answer with exactly one of:
- I suspect
- Hard to tell
- No suspicion)

Q14 If you suspected AI generation or manipulation, what clues led you to this judgment?
(If your answer to Q13 was "No suspicion", answer "N/A".)
(Free-text explanation in English.)

Q15 Did seeing similar claims before affect your judgment? How?
(Free-text explanation in English.)

Q16 How might personal feelings influence interpretation of this statement?
(Free-text explanation in English.)
"""

# =========================
# 2) Define "View Image and Answer Questionnaire" Signature ‚Äî field name must be image
# =========================
class SurveyVisionTask(dspy.Signature):
    """
    You are an expert survey annotator.

    HARD RULES:
    - You MUST answer Q1 ~ Q16 in ENGLISH only.
    - Do NOT use Chinese or any non-English words.
    - Output format: Q1: ... Q2: ... ... Q16: ...
      No extra commentary before or after.
    """
    image = dspy.InputField(desc="image to look at (dspy.Image or URL)")
    claim = dspy.InputField(desc="claim text")
    questions = dspy.InputField(desc="full questions Q1~Q16")
    answers = dspy.OutputField(desc="answers from Q1 to Q16, one per line, prefixed with Qx:")

# =========================
# 3) Utility functions for parsing Q1~Q16
# =========================
def parse_qa_block(text: str):
    qa_dict = {}
    if not text:
        return qa_dict
    pattern = r"(Q\d+)\s*[:\-]\s*(.*?)(?=(?:\nQ\d+\s*[:\-])|$)"
    for m in re.finditer(pattern, text, flags=re.DOTALL):
        qid = m.group(1).strip()   # 'Q7'
        ans = m.group(2).strip()   # Answer for that question (may be multi-line)
        qa_dict[qid] = ans
    return qa_dict

def answers_to_pipe_row(answer_text: str, model_name: str, task_number):
    """
    Convert to: PROLIFIC_PID | task_number | Q1 | ... | Q16
    (For you to copy into spreadsheet)
    """
    qa_dict = parse_qa_block(answer_text)
    cols = [model_name, str(task_number)]
    for i in range(1, 17):
        qid = f"Q{i}"
        cols.append(qa_dict.get(qid, ""))
    return " | ".join(cols)

def answers_to_record(answer_text: str, model_name: str, task_number):
    """
    Convert to a dict for final CSV writing:
    { "PROLIFIC_PID": ..., "Statement_ID": ..., "Q1": ..., ... "Q16": ... }
    """
    qa_dict = parse_qa_block(answer_text)
    rec = {
        "PROLIFIC_PID": model_name,
        "Statement_ID": task_number,
    }
    for i in range(1, 17):
        qid = f"Q{i}"
        rec[qid] = qa_dict.get(qid, "")
    return rec

# =========================
# 4) Configure models to run
# =========================
MODELS = [
    {"name": "GPT-5.1",          "api_model": "openrouter/openai/gpt-5.1"},
    {"name": "Gemini-2.5-Pro",   "api_model": "openrouter/google/gemini-2.5-pro"},
    {"name": "Claude-Opus-4.5",  "api_model": "openrouter/anthropic/claude-opus-4.5"},
    {"name": "Llama-4-Maverick", "api_model": "openrouter/meta-llama/llama-4-maverick"},
    {"name": "Grok-4-fast",      "api_model": "openrouter/x-ai/grok-4-fast"},
]

# =========================
# 5) Your 5 sets of questions (Excel)
# =========================
EXCEL_FILES = ["test4.1.xlsx", "test5.xlsx", "test6.xlsx", "test7.xlsx", "test8.xlsx"]

# =========================
# 6) Run: For each Excel √ó each model √ó each task (timing + printing + saving CSV)
# =========================
api_key = os.environ.get("OPENROUTER_API_KEY")
if not api_key:
    raise EnvironmentError("OPENROUTER_API_KEY environment variable not found. Please set it first.")

print("PROLIFIC_PID | excel_file | task_number | Q1 | Q2 | Q3 | Q4 | Q5 | Q6 | Q7 | Q8 | Q9 | Q10 | Q11 | Q12 | Q13 | Q14 | Q15 | Q16 | elapsed_sec")
print("-" * 180)

all_records = []

for EXCEL_FILE in EXCEL_FILES:
    df = pd.read_excel(EXCEL_FILE)
    print(f"\nüìò Reading question file: {EXCEL_FILE}")
    print("Current column names:", list(df.columns))

    for col in ["task_number", "claim", "image_url"]:
        if col not in df.columns:
            raise ValueError(f"Excel missing column: {col} (file: {EXCEL_FILE})")

    TASKS = [
        {
            "task_number": row["task_number"],
            "claim": row["claim"],
            "image_url": row["image_url"],
        }
        for _, row in df.iterrows()
    ]
    print(f"‚úÖ Read {len(TASKS)} tasks from {EXCEL_FILE}.")

    for model_cfg in MODELS:
        model_name = model_cfg["name"]
        api_model = model_cfg["api_model"]

        print(f"\n==================== Now starting: {model_name} | {EXCEL_FILE} ({api_model}) ====================\n")

        # Total timing for each model on each Excel
        suite_start = time.perf_counter()

        # Configure dspy LM
        lm = dspy.LM(
            model=api_model,
            api_base="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        dspy.settings.configure(lm=lm)
        program = dspy.ChainOfThought(SurveyVisionTask)

        for task in TASKS:
            tn = task["task_number"]
            claim = task["claim"]
            image_url = task["image_url"]

            # Read image (allow fallback on failure)
            try:
                img_obj = dspy.Image(image_url, download=True)
            except Exception as e:
                print(f"‚ö†Ô∏è Task {tn} failed to download image, falling back to URL string: {e}")
                img_obj = image_url

            # Single task timing
            t0 = time.perf_counter()
            resp = program(image=img_obj, claim=claim, questions=QUESTIONS_TEXT)
            t1 = time.perf_counter()
            elapsed = t1 - t0

            answer_text = resp.answers if getattr(resp, "answers", None) is not None else ""

            # Pipe row: your original output (add excel_file + elapsed)
            pipe = answers_to_pipe_row(answer_text, model_name, tn)  # model | task | Q1..Q16
            pipe_parts = pipe.split(" | ")
            # pipe_parts: [model, task_number, Q1..Q16]
            # We print as: model | excel | task | Q1..Q16 | elapsed
            print(
                f"{pipe_parts[0]} | {EXCEL_FILE} | {pipe_parts[1]} | "
                + " | ".join(pipe_parts[2:])
                + f" | {elapsed:.2f}"
            )

            # Save structured record (for CSV)
            rec = answers_to_record(answer_text, model_name, tn)
            rec["excel_file"] = EXCEL_FILE
            rec["api_model"] = api_model
            rec["image_url"] = image_url
            rec["claim"] = claim
            rec["elapsed_sec"] = round(elapsed, 4)
            all_records.append(rec)

        suite_end = time.perf_counter()
        print(f"\n‚è±Ô∏è {model_name} completed {EXCEL_FILE} total time: {suite_end - suite_start:.2f} seconds\n")

# =========================
# 7) Write CSV (detailed + summary)
# =========================
df_out = pd.DataFrame(all_records)

out_file = "model_responses.csv"
df_out.to_csv(out_file, index=False, encoding="utf-8-sig")
print(f"\n‚úÖ Detailed results saved as {out_file}")

summary = (
    df_out.groupby(["excel_file", "PROLIFIC_PID"], as_index=False)["elapsed_sec"]
         .agg(total_sec="sum", avg_sec="mean", max_sec="max", min_sec="min", n_tasks="count")
         .sort_values(["excel_file", "total_sec"], ascending=[True, False])
)

summary_file = "timing_summary.csv"
summary.to_csv(summary_file, index=False, encoding="utf-8-sig")
print(f"‚úÖ Timing summary saved as {summary_file}")
print(summary)