# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v9Qu92XBHTKFHNXNMJfIyQok8eGC3twy
"""

# @title 1. Install Dependencies and Configure API
!pip install openai pandas matplotlib seaborn tqdm openpyxl

import os
import glob
import pandas as pd
import json
import time
from openai import OpenAI
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# ================= Configuration Area =================
# Enter your OpenRouter API Key
OPENROUTER_API_KEY = "" # @param {type:"string"}
API_MODEL = "openai/gpt-5.1" # Or use "openai/gpt-4o" etc.
SITE_URL = "https://colab.research.google.com"
APP_NAME = "LogicAnalysis"
# ===========================================

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
    default_headers={
        "HTTP-Referer": SITE_URL,
        "X-Title": APP_NAME,
    }
)

print("Environment configuration completed.")

# @title 1. (Forced Q6/Q7 Version) Read Model/Human/Task Data + Majority Voting Grouping + Reasoning Only from Q6/Q7
import pandas as pd
import numpy as np
import re

MODEL_PATH = "/content/æ¨¡åž‹ç­”æ¡ˆ.xlsx"
HUMAN_PATH  = "/content/äººç±»æ ‡æ³¨è€…ç­”æ¡ˆ.xlsx"
TASK_PATH   = "/content/é¢˜ç›®.xlsx"

def normalize_split(name):
    m = re.search(r"(test\s*\d+)", str(name).lower())
    return m.group(1).replace(" ", "") if m else "unknown"

def normalize_columns(df):
    df = df.copy()
    df.columns = df.columns.astype(str).str.strip()

    # Unify Statement_ID
    if "Statement_ID" not in df.columns:
        for c in df.columns:
            if str(c).strip().lower() in ["statement_id","task_id","task_num","id","no","index","question_id"]:
                df.rename(columns={c:"Statement_ID"}, inplace=True)
                break

    return df

def load_xlsx_all_sheets(path, source_name):
    xls = pd.read_excel(path, sheet_name=None)
    frames = []
    for sheet, df in xls.items():
        if df is None or len(df)==0:
            continue
        df = normalize_columns(df)
        df["Split"] = normalize_split(sheet)

        # Force requirement for ID
        if "Statement_ID" not in df.columns:
            continue

        # Convert ID to int
        df["Statement_ID"] = pd.to_numeric(df["Statement_ID"], errors="coerce").astype("Int64")

        # âœ… Force Reasoning from Q6 / Q7 (empty if doesn't exist)
        q6 = df["Q6"] if "Q6" in df.columns else ""
        q7 = df["Q7"] if "Q7" in df.columns else ""

        q6 = q6.fillna("").astype(str)
        q7 = q7.fillna("").astype(str)

        # Merge (keep original columns)
        df["Reasoning_Q6"] = q6
        df["Reasoning_Q7"] = q7
        df["Reasoning_Q6Q7"] = (q6.str.strip() + " " + q7.str.strip()).str.strip()

        # Compatibility for later module2/module3 using Reasoning directly
        df["Reasoning"] = df["Reasoning_Q6Q7"]

        df["Source"] = source_name
        frames.append(df)

    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()

def load_tasks(task_path):
    xls = pd.read_excel(task_path, sheet_name=None)
    frames = []
    for sheet, df in xls.items():
        if df is None or len(df)==0:
            continue
        df = normalize_columns(df)
        df["Split"] = normalize_split(sheet)

        # In task sheet, ID is called task_num
        if "Statement_ID" not in df.columns and "task_num" in [c.lower() for c in df.columns]:
            # normalize_columns already changes task_num to Statement_ID (if matched)
            pass

        if "Statement_ID" not in df.columns:
            continue

        # Unify label
        if "label" not in df.columns:
            for c in df.columns:
                if str(c).strip().lower() in ["label","gt","ground_truth","answer"]:
                    df.rename(columns={c:"label"}, inplace=True)
                    break
        if "label" not in df.columns:
            continue

        df["Statement_ID"] = pd.to_numeric(df["Statement_ID"], errors="coerce").astype("Int64")
        df["label"] = pd.to_numeric(df["label"], errors="coerce").astype("Int64")

        frames.append(df[["Split","Statement_ID","label"]])

    out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    out = out.dropna(subset=["Split","Statement_ID","label"])
    out = out.drop_duplicates(subset=["Split","Statement_ID"])
    return out

def parse_label_from_Q5(df):
    # Q5: "Checkworthy" / "Not checkworthy" (according to your previous logic)
    if "Q5" not in df.columns:
        return pd.Series([0]*len(df))
    s = df["Q5"].fillna("").astype(str).str.lower()
    return s.apply(lambda x: 1 if ("checkworthy" in x and "not" not in x) else 0)

def load_data_q6q7_only():
    print("=== Module1: Read Data (Reasoning forced from Q6/Q7 only) ===")

    df_m = load_xlsx_all_sheets(MODEL_PATH, "Model")
    df_h = load_xlsx_all_sheets(HUMAN_PATH, "Human")
    df_t = load_tasks(TASK_PATH)

    print(f"Model rows: {len(df_m)} | Human rows: {len(df_h)} | Tasks rows: {len(df_t)}")

    # GT map
    gt_map = df_t.set_index(["Split","Statement_ID"])["label"].to_dict()

    # Label_Val
    df_m["Label_Val"] = parse_label_from_Q5(df_m)
    df_h["Label_Val"] = parse_label_from_Q5(df_h)

    # Grouping (by Split + Statement_ID, majority voting for model/human separately)
    groups = []
    keys = set(df_m[["Split","Statement_ID"]].dropna().itertuples(index=False, name=None))
    for split, sid in keys:
        gt = gt_map.get((split, sid), None)
        if gt is None:
            continue

        m_rows = df_m[(df_m["Split"]==split) & (df_m["Statement_ID"]==sid)]
        h_rows = df_h[(df_h["Split"]==split) & (df_h["Statement_ID"]==sid)]
        if len(m_rows)==0 or len(h_rows)==0:
            continue

        m_win = (m_rows["Label_Val"] == int(gt)).mean() > 0.5
        h_win = (h_rows["Label_Val"] == int(gt)).mean() > 0.5

        if m_win and h_win: base = "Both Right"
        elif (not m_win) and (not h_win): base = "Both Wrong"
        elif m_win and (not h_win): base = "Only Human Wrong"
        else: base = "Only Human Right"

        groups.append({"Split": split, "Statement_ID": sid, "User_Group": f"{base} (GT={int(gt)})"})

    df_g = pd.DataFrame(groups)

    # Merge into df_individual
    df_final = pd.merge(
        pd.concat([df_m, df_h], ignore_index=True),
        df_g,
        on=["Split","Statement_ID"],
        how="inner"
    )

    # Health check: Verify Reasoning truly comes from Q6/Q7
    blank = (df_final["Reasoning"].fillna("").astype(str).str.strip() == "").mean()
    print("\nðŸ” Data Health Check:")
    print(f"  df_individual rows: {len(df_final)}")
    print(f"  Reasoning(Q6Q7) blank rate: {blank:.2%}")
    print("\nðŸ“Š Group counts:")
    print(df_final["User_Group"].value_counts())

    # Sample to check, confirm no Q8-Q16 content (manual judgment)
    print("\nðŸ§ª Sample Reasoning (first 5):")
    for x in df_final["Reasoning"].head(5).tolist():
        print("-", x[:160])

    return df_final

df_individual = load_data_q6q7_only()

# Check if Reasoning matches Q6/Q7 merge
mismatch = (df_individual["Reasoning"].fillna("").astype(str).str.strip()
            != df_individual["Reasoning_Q6Q7"].fillna("").astype(str).str.strip()).mean()
print("Reasoning != Reasoning_Q6Q7 mismatch rate:", mismatch)

# Sample check for "question number traces" in Reasoning (generally shouldn't appear)
suspicious = df_individual["Reasoning"].fillna("").astype(str).str.contains(r"\bQ(8|9|1[0-6])\b", case=False).mean()
print("Contains 'Q8-Q16' token rate:", suspicious)

# @title 2. (Cleaned Fix Version) Generate "Natural Language" and "Comprehensive" Codebook
import json
from openai import OpenAI
import re

# âš ï¸ Ensure KEY exists
if 'OPENROUTER_API_KEY' not in locals():
    # OPENROUTER_API_KEY = "your KEY"
    pass

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
    default_headers={"HTTP-Referer": "https://colab.research.google.com", "X-Title": "CodebookGen"}
)

def generate_codebook_human_logic(df):
    print("=== 2. Start Generating Codebook (Goal: Natural Human Language + Complete Logic Coverage) ===")
    if df is None: return []

    # 1. Prepare full data
    if 'Reasoning' not in df.columns:
        print("âŒ Error: 'Reasoning' column not found!")
        return []

    texts = df['Reasoning'].dropna().astype(str).tolist()
    print(f"ðŸ“š Deeply analyzing {len(texts)} reasoning texts...")

    text_block = ""
    for i, t in enumerate(texts):
        clean_t = t.replace("\n", " ").strip()[:500]
        text_block += f"[{i+1}] {clean_t}\n"

    # 2. Core Prompt: Natural human language + forced coverage of Fact/Irrelevant
    prompt = """
    You are a Human Annotator explaining your "gut feeling" about why a post needs fact-checking or not.

    Task:
    Summarize 10-12 "Core Reasoning Patterns" from the dataset.

    CRITICAL REQUIREMENT 1 (TONE):
    - **Speak like a human**, not a textbook.
    - âŒ BAD: "Risk of Reputation Damage", "High Subjectivity", "Verifiability Issues".
    - âœ… GOOD: "Could hurt someone's reputation", "Purely personal opinion", "Hard to verify".
    - Use phrases starting with verbs or adjectives: "Is...", "Could...", "Might...", "Sounds...".

    CRITICAL REQUIREMENT 2 (COVERAGE):
    You MUST cover logic for BOTH "Check-Worthy" (Yes) and "Not Check-Worthy" (No).
    Specifically, ensure you have categories for:
    - **Facts**: e.g., "Is a well-known fact" or "Basic common knowledge".
    - **Irrelevance**: e.g., "Totally irrelevant content" or "Has no meaning".
    - **Humor**: e.g., "Just a joke or satire".
    - **Opinion**: e.g., "Just personal feelings".
    - **Harm**: e.g., "Could mislead the public", "Promotes hate/violence".

    Output strictly a JSON object:
    {
      "categories": [
        "Natural Phrase 1",
        "Natural Phrase 2",
        ...
      ]
    }
    """

    try:
        resp = client.chat.completions.create(
            model="openai/gpt-4o",
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": text_block}],
            response_format={"type": "json_object"},
            temperature=0.4
        )
        data = json.loads(resp.choices[0].message.content)
        cats = data.get('categories', [])

        # 3. Clean
        clean_cats = []
        for c in cats:
            c_clean = re.sub(r'\(.*?\)', '', str(c)).strip() # Remove parentheses
            c_clean = c_clean.split(':')[0].strip() # Remove colon
            if len(c_clean) > 3:
                clean_cats.append(c_clean)

        # 4. Fallback check (if AI still missed, manually add)
        # Check for "Fact" related
        if not any(k in str(clean_cats).lower() for k in ['fact', 'known', 'knowledge', 'true']):
            clean_cats.append("Is a well-known fact")
        # Check for "Irrelevant" related
        if not any(k in str(clean_cats).lower() for k in ['irrelevant', 'impact', 'care', 'matter']):
            clean_cats.append("Totally irrelevant")
        # Check Other
        if "Other / Unclear" not in clean_cats:
            clean_cats.append("Other / Unclear")

        print(f"âœ… Codebook generated successfully (Total {len(clean_cats)} categories):")
        for i, c in enumerate(clean_cats):
            print(f"  {i+1}. {c}")

        return clean_cats

    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        # Fallback plan
        return [
            "Could mislead the public",
            "Could hurt someone's reputation",
            "Sounds too fake to be true",
            "Is a well-known fact",
            "Just a joke or satire",
            "Purely personal opinion",
            "Totally irrelevant",
            "Promotes hate or violence",
            "Hard to verify",
            "Other / Unclear"
        ]

# Execute
GLOBAL_CODEBOOK = generate_codebook_human_logic(df_individual)

# @title 3. (Fixed Other Version) Stable Full Extraction: No ITEM_0 Loss + Allow Short Name Matching Codebook
import json, re
import pandas as pd
from tqdm import tqdm

YOUR_MODEL_NAME = "openai/gpt-4o"   # Recommended to use 4o for most stable extraction

def _safe_get(d: dict, keys, default=""):
    for k in keys:
        if k in d and d[k] is not None:
            return d[k]
    return default

def build_codebook_alias(codebook):
    """
    For each codebook item (could be 'Name (Definition)'), create aliases:
    - Full string (lowercase) -> original string
    - Short name before parentheses (lowercase) -> original string
    So model returning short names can map back to full codebook items, won't be classified as Other
    """
    alias = {}
    for full in codebook:
        full = str(full).strip()
        if not full:
            continue
        short = re.split(r"\(", full)[0].strip()  # Before parentheses
        alias[full.lower()] = full
        if short:
            alias[short.lower()] = full
    return alias

def normalize_category(cat_raw, alias_map):
    """
    Allow:
    - Full match
    - Short name (before parentheses) match
    - Case insensitive
    """
    c = str(cat_raw).strip()
    if not c:
        return "Other"
    c_low = c.lower()
    c_short = re.split(r"\(", c)[0].strip().lower()

    if c_low in alias_map:
        return alias_map[c_low]
    if c_short in alias_map:
        return alias_map[c_short]
    return "Other"

def extract_logic_stable_no_missing_fixed_other(df, codebook, batch_size=12, max_text_chars=550):
    print(f"=== 3. Stable Extraction (Fixed Other) (Analysis Model: {YOUR_MODEL_NAME}) ===")
    if df is None or len(df) == 0:
        print("âŒ df is empty")
        return None

    # Required columns check
    need = ["Split","Statement_ID","User_Group","Source","Reasoning"]
    for c in need:
        if c not in df.columns:
            raise ValueError(f"df missing column: {c}")

    work = df[need].copy()
    work["Reasoning"] = work["Reasoning"].fillna("").astype(str)

    blank_rate = (work["Reasoning"].str.strip() == "").mean()
    print(f"ðŸ”Ž Reasoning blank text ratio: {blank_rate:.2%}")

    # âœ… Build alias mapping (key fix point)
    alias_map = build_codebook_alias(codebook)

    records = work.to_dict("records")
    results = []

    # âœ… Prompt also slightly relaxed: allow outputting short names (before parentheses), we'll map back locally
    system_prompt = f"""
You are an expert Data Coder.

Codebook (choose EXACTLY ONE category):
{json.dumps(codebook, ensure_ascii=False)}

IMPORTANT:
- You may output either the full category string OR just the category NAME before parentheses.
  (We will map the short name back to the full codebook item.)

Input: a JSON array. Each item has idx and text.

For EACH item:
1) Pick ONE Category from the codebook.
2) Extract Evidence as 2â€“5 words copied VERBATIM from the text.
   - Evidence must be copied, not paraphrased.
   - If text is blank/too short: Category="Other", Evidence="".

CRITICAL OUTPUT FORMAT (valid JSON only):
{{"items":[{{"idx":0,"Category":"...","Evidence":"..."}}, ...]}}
Rules:
- items length MUST equal input length
- Every idx from input MUST appear exactly once
"""

    for start in tqdm(range(0, len(records), batch_size)):
        batch = records[start:start+batch_size]

        payload = []
        for j, item in enumerate(batch):
            txt = re.sub(r"\s+", " ", str(item.get("Reasoning",""))).strip()
            txt = txt[:max_text_chars]
            payload.append({"idx": j, "text": txt})

        try:
            resp = client.chat.completions.create(
                model=YOUR_MODEL_NAME,
                messages=[
                    {"role":"system","content":system_prompt},
                    {"role":"user","content":json.dumps(payload, ensure_ascii=False)}
                ],
                response_format={"type":"json_object"},
                temperature=0.0
            )
            out = json.loads(resp.choices[0].message.content)

            items = out.get("items", None)
            if not isinstance(items, list) or len(items) != len(payload):
                raise ValueError(f"Bad output structure: items_len={0 if items is None else len(items)} expected={len(payload)}")

            # idx -> obj
            out_map = {}
            for obj in items:
                if isinstance(obj, dict) and "idx" in obj:
                    out_map[int(obj["idx"])] = obj

            # Complete each entry
            for j, item in enumerate(batch):
                obj = out_map.get(j, {})

                cat_raw = _safe_get(obj, ["Category","category","Logic_Category","label"], "Other")
                ev  = _safe_get(obj, ["Evidence","evidence","Evidence_Keywords","Evidence Keywords","keywords"], "")

                ev  = re.sub(r"\s+", " ", str(ev)).strip()

                # âœ… Use alias mapping to fix Other
                cat = normalize_category(cat_raw, alias_map)

                if len(ev) > 80:
                    ev = ev[:80] + "..."

                results.append({
                    "Split": item.get("Split"),
                    "Statement_ID": item.get("Statement_ID"),
                    "Group": item.get("User_Group"),
                    "Source": item.get("Source"),
                    "Logic_Category": cat,
                    "Evidence_Keywords": ev,
                    "Category_Raw": str(cat_raw).strip()  # âœ… Optional: keep original output for debugging
                })

        except Exception as e:
            for item in batch:
                results.append({
                    "Split": item.get("Split"),
                    "Statement_ID": item.get("Statement_ID"),
                    "Group": item.get("User_Group"),
                    "Source": item.get("Source"),
                    "Logic_Category": "Error",
                    "Evidence_Keywords": f"Error: {str(e)[:140]}",
                    "Category_Raw": "Error"
                })

    df_res = pd.DataFrame(results)
    df_res.to_csv("final_extraction_results.csv", index=False)
    print(f"\nâœ… Extraction completed: {len(df_res)} rows -> final_extraction_results.csv")

    empty_ev = (df_res["Evidence_Keywords"].fillna("").astype(str).str.strip() == "").mean()
    err_rate = (df_res["Logic_Category"] == "Error").mean()
    other_rate = (df_res["Logic_Category"] == "Other").mean()
    print(f"ðŸ” Empty Evidence ratio: {empty_ev:.2%}")
    print(f"ðŸ§¯ Error ratio: {err_rate:.2%}")
    print(f"ðŸ“¦ Other ratio: {other_rate:.2%}")
    print("ðŸ“Œ Category Top10:")
    print(df_res["Logic_Category"].value_counts().head(10))

    return df_res

# Execute
df_results = extract_logic_stable_no_missing_fixed_other(df_individual, GLOBAL_CODEBOOK)

# @title 4. (Plotting) Aggregation Statistics and Original Keyword Display
import matplotlib.pyplot as plt
import seaborn as sns

def plot_final_keywords(flat_df):
    print("=== 4. Starting Aggregation and Plotting ===")
    if flat_df is None: return

    # 1. Aggregation
    # Merge Evidence Keywords (deduplicate)
    def agg_keywords(series):
        # Split comma-separated words, deduplicate, sort
        all_words = []
        for s in series:
            if s and str(s) != 'nan' and s != 'Error':
                # Assume model returns "keyword1, keyword2"
                words = [w.strip() for w in str(s).split(',')]
                all_words.extend(words)

        # Count highest frequency words, or just list top 5 distinct words
        unique_words = sorted(list(set(all_words)), key=len) # Sort by length, show short words first
        return " | ".join(unique_words[:4]) # Show only first 4 shortest words

    df_agg = flat_df.groupby(['Group', 'Source', 'Logic_Category']).agg({
        'Statement_ID': 'count',
        'Evidence_Keywords': agg_keywords
    }).reset_index().rename(columns={'Statement_ID': 'Count'})

    df_agg = df_agg.sort_values(['Group', 'Source', 'Count'], ascending=[True, True, False])
    df_agg.to_csv("final_summary_keywords.csv", index=False)
    print("âœ… Statistics table generated: final_summary_keywords.csv")

    # 2. Plotting
    TARGET_GROUPS = sorted(flat_df['Group'].unique())

    for group in TARGET_GROUPS:
        subset = df_agg[df_agg['Group'] == group]
        if subset.empty: continue

        n_human = flat_df[(flat_df['Group'] == group) & (flat_df['Source'] == 'Human')].shape[0]
        n_model = flat_df[(flat_df['Group'] == group) & (flat_df['Source'] == 'Model')].shape[0]

        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle(f"Logic Analysis: {group}", fontsize=16, fontweight='bold')

        def draw(ax, source, n_val, color):
            data = subset[subset['Source'] == source].head(10)
            if len(data) > 0:
                sns.barplot(data=data, y='Logic_Category', x='Count', ax=ax, palette=color)
                ax.set_title(f"{source} (n={n_val})")
                ax.set_xlabel("Frequency")

                print(f"\n>>> [{group}] {source} Core Evidence Keywords:")
                for _, r in data.head(5).iterrows():
                    # Print format: Logic Category (Count) -> [Keyword1 | Keyword2]
                    print(f"  â€¢ {r['Logic_Category']} ({r['Count']}): [{r['Evidence_Keywords']}]")
            else:
                ax.text(0.5, 0.5, "No Data", ha='center')

        draw(axes[0], "Human", n_human, "magma")
        draw(axes[1], "Model", n_model, "viridis")

        plt.tight_layout()
        plt.show()

plot_final_keywords(df_results_keywords)